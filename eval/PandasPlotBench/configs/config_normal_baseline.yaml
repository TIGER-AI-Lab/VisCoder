run_mode: "normal"
paths:
  out_folder: "eval_results/baseline/matplotlib/" # for output files
  # out_folder: "eval_results/baseline_qwen3_best_practice/plotly/" # for output files
  dataset_folder: "dataset"
  results_filename: "results.json" # json stores temporarily plotting responses. Located in the out_folder
  bench_stat_filename: "benchmark_stat.jsonl" # json stores final statistics of the benchmark results. Located in the out_folder
  error_rate_file: "eval_results/baseline.json"
  # instructs_file: "instructs/instructs.json" # Optional. Default instructs exists. json stores instructs for plot generation and LLM-benchmarking.
benchmark_types: ["vis", "task"] # Options: "vis", "task", "codebert"
plotting_lib: "matplotlib" # "matplotlib", "seaborn", "plotly", "lets-plot" (does not work)
data_descriptor: "head" # data descriptor. Options: "pycharm", "datalore", "lida", "head", "describe", "empty"
model_plot_gen:
  names: []
  parameters: # List of additional model parameters.
    temperature: 0.0
model_judge:
  name: "openai/gpt-4o-2024-05-13"
  parameters: # List of additional model parameters.
    temperature: 0.0