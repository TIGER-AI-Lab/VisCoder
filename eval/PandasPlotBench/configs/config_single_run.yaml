run_mode: "self_debug" # e.g. normal or self_debug
debug:
  top_k: 3
  output_dir: "debug_results/single_run_test/matplotlib/"
paths:
  out_folder: "eval_results/single_run_test/matplotlib/" # for output files
  dataset_folder: "dataset"
  results_filename: "results.json" # json stores temporarily plotting responses. Located in the out_folder
  bench_stat_filename: "benchmark_stat.jsonl" # json stores final statistics of the benchmark results. Located in the out_folder
  error_rate_file: "eval_results/single_run_test.json"
  # instructs_file: "instructs/instructs.json" # Optional. Default instructs exists. json stores instructs for plot generation and LLM-benchmarking.
benchmark_types: ["vis", "task"] # Options: "vis", "task", "codebert"
plotting_lib: "matplotlib" # "matplotlib", "seaborn", "plotly", "lets-plot" (does not work)
data_descriptor: "head" # data descriptor. Options: "pycharm", "datalore", "lida", "head", "describe", "empty"
model_plot_gen:
  names: [
    "TIGER-Lab/VisCoder-7B"
    ]
  parameters: # List of additional model parameters.
    temperature: 0.0
model_judge:
  name: "openai/gpt-4o-2024-05-13"
  parameters: # List of additional model parameters.
    temperature: 0.0